"""
Analyzes and synthesizes pre-processed connectome data into a single CSV file
called data/processed/aggregated_connectome.csv.

This script reads pre-processed connectome files (graph_tensors_*.pt) generated by
preprocess.py -c and combines their data into an aggregated_connectome.csv file.
It extracts connection information from each tensor file, calculates statistics
for gap junctions, chemical synapses, and functional weights, and merges them.

The output CSV contains the following columns:
- neuron1, neuron2: The pair of neurons (directed from neuron1 to neuron2)
- mean_gap_weight: Average gap junction weight (symmetric)
- mean_chem_weight: Average chemical synapse weight (directed)
- functional_weight: Functional connectivity weight from the funconn dataset
- data_sources: List of source datasets that contributed to this connection
"""

import torch
import pandas as pd
import numpy as np
import os
import glob
from collections import defaultdict


# --- Configuration ---
PROCESSED_DATA_DIR = "./data/processed/connectome"
from preprocess._pkg import NEURON_LABELS


# --- Helper Mappings ---
if not NEURON_LABELS:
    raise ValueError("NEURON_LABELS list is empty. Cannot proceed.")
NEURON_TO_IDX = {label: idx for idx, label in enumerate(NEURON_LABELS)}
IDX_TO_LABEL = {idx: label for label, idx in NEURON_TO_IDX.items()}
NUM_NODES = len(NEURON_LABELS)

# --- Functions ---

def get_dataset_name(filepath: str) -> str:
    """Extracts a base name from the .pt filepath for use as data source identifier."""
    base = os.path.basename(filepath)
    name = base.replace("graph_tensors_", "").replace(".pt", "")
    return name

def analyze_all_connectomes(data_dir: str) -> pd.DataFrame:
    """
    Loads all processed .pt connectome files, aggregates connection data,
    and returns a summary DataFrame.

    Args:
        data_dir: Path to the directory containing the .pt files.

    Returns:
        A Pandas DataFrame summarizing connections across datasets.
        Columns: neuron1, neuron2, data_sources, mean_gap_weight,
                 mean_chem_weight, functional_weight.
    """
    pt_files = glob.glob(os.path.join(data_dir, "graph_tensors_*.pt"))
    if not pt_files:
        print(f"Warning: No 'graph_tensors_*.pt' files found in {data_dir}")
        return pd.DataFrame(
            columns=[
                "neuron1",
                "neuron2",
                "mean_gap_weight",
                "mean_chem_weight",
                "functional_weight",
                "data_sources",
            ]
        )

    # Store collected data per DIRECTED pair (neuron1_label, neuron2_label)
    # will calculate the symmetric gap mean later
    collected_data = defaultdict(lambda: {
        "gap_vals_A_to_B": [], # Raw gap A->B from each source
        "gap_vals_B_to_A": [], # Raw gap B->A from each source (used for symmetric mean)
        "chem_vals_A_to_B": [], # Raw chem A->B from each source
        "func_val_A_to_B": np.nan, # Specific func A->B from funconn source
        "sources": set()
    })

    print(f"Found {len(pt_files)} potential connectome files to analyze.")

    # --- Step 1: Load data and collect raw values per directed edge ---
    for filepath in pt_files:
        dataset_name = get_dataset_name(filepath)
        print(f"Processing: {dataset_name} ({os.path.basename(filepath)})")
        try:
            data = torch.load(filepath, map_location=torch.device('cpu')) # Load to CPU
            edge_index = data.get('edge_index')
            edge_attr = data.get('edge_attr')

            if edge_index is None or edge_attr is None:
                print(f"  Skipping {dataset_name}: Missing 'edge_index' or 'edge_attr'.")
                continue
            if edge_attr.shape[1] < 2:
                 print(f"  Skipping {dataset_name}: edge_attr has shape {edge_attr.shape}, expected at least 2 features.")
                 continue

            # Create an efficient lookup for edges in this specific file
            # Assumes indices in the file match global NEURON_TO_IDX
            edge_lookup = {
                (u.item(), v.item()): i
                for i, (u, v) in enumerate(edge_index.t())
            }

            # Iterate through all possible neuron pairs using global indices
            for idx1 in range(NUM_NODES):
                for idx2 in range(NUM_NODES):
                    # Get labels for the current pair
                    label1 = IDX_TO_LABEL.get(idx1)
                    label2 = IDX_TO_LABEL.get(idx2)
                    if not label1 or not label2: continue # Should not happen

                    pair_key = (label1, label2) # Directed key

                    # Check if the directed edge A->B exists in this dataset
                    edge_map_idx = edge_lookup.get((idx1, idx2))
                    if edge_map_idx is not None:
                        attr = edge_attr[edge_map_idx]
                        gap_w = attr[0].item()
                        chem_w = attr[1].item()

                        collected_data[pair_key]["sources"].add(dataset_name)
                        if gap_w != 0:
                            collected_data[pair_key]["gap_vals_A_to_B"].append(gap_w)
                        if chem_w != 0:
                            collected_data[pair_key]["chem_vals_A_to_B"].append(chem_w)

                        # Special handling for functional connectivity dataset
                        if dataset_name == "funconn" and chem_w != 0:
                            # Randi2023 uses the chemical slot for functional weight
                            collected_data[pair_key]["func_val_A_to_B"] = chem_w

                    # Also check the reverse edge B->A to collect its gap weight for symmetric calculation later
                    edge_map_idx_rev = edge_lookup.get((idx2, idx1))
                    if edge_map_idx_rev is not None:
                        attr_rev = edge_attr[edge_map_idx_rev]
                        gap_w_rev = attr_rev[0].item()
                        if gap_w_rev != 0:
                             collected_data[pair_key]["gap_vals_B_to_A"].append(gap_w_rev)


        except Exception as e:
            print(f"  Error processing {dataset_name}: {e}")

    # --- Step 2: Aggregate collected data into final DataFrame rows ---
    output_rows = []
    processed_pairs = set() # Keep track to avoid duplicating symmetric gap info logic

    print("\nAggregating results...")
    # Iterate through all N*N pairs again to ensure all are included
    for label1 in NEURON_LABELS:
        for label2 in NEURON_LABELS:
            # include self-loops
            pair_key_AB = (label1, label2)
            pair_key_BA = (label2, label1)
            data_AB = collected_data[pair_key_AB]
            data_BA = collected_data[pair_key_BA] # Get data collected for the reverse direction
            # Combine all gap values related to the pair {A, B} from both directions
            all_gap_values = data_AB["gap_vals_A_to_B"] + data_AB["gap_vals_B_to_A"]
            # We might have duplicates if a dataset correctly listed symmetric values, use unique values
            unique_gap_values = list(set(all_gap_values))

            mean_gap = np.mean(unique_gap_values) if unique_gap_values else np.nan

            # Calculate mean directed chemical weight A->B
            mean_chem_AB = np.mean(data_AB["chem_vals_A_to_B"]) if data_AB["chem_vals_A_to_B"] else np.nan

            # Get directed functional weight A->B
            func_AB = data_AB["func_val_A_to_B"]

            # Get combined sources (any dataset mentioning A->B or B->A contributes)
            sources_AB = sorted(list(data_AB["sources"]))

            # Only add row if there's *any* evidence for this directed connection
            if sources_AB or not np.isnan(func_AB):
                 output_rows.append({
                     "neuron1": label1,
                     "neuron2": label2,
                     "mean_gap_weight": mean_gap, # Symmetric mean
                     "mean_chem_weight": mean_chem_AB, # Directed mean A->B
                     "functional_weight": func_AB, # Directed A->B
                     "data_sources": sources_AB
                 })

    print(f"Created {len(output_rows)} summary rows.")
    if not output_rows:
        return pd.DataFrame(
            columns=[
                "neuron1",
                "neuron2",
                "mean_gap_weight",
                "mean_chem_weight",
                "functional_weight",
                "data_sources",
            ]
        )

    return pd.DataFrame(output_rows)


if __name__ == "__main__":
    print("Starting connectome analysis...")
    summary_df = analyze_all_connectomes(PROCESSED_DATA_DIR)

    if not summary_df.empty:
        print("\nAnalysis Complete. DataFrame head:")
        print(summary_df.head(100))
        
        # --- Calculate and Print Statistics ---
        print("\n--- Overall Statistics ---")
        print(f"Total Neuron Pairs:{len(summary_df)}\n")

        # Gap Junctions (Based on unique pairs)
        gap_df = summary_df.dropna(subset=["mean_gap_weight"])
        # Create a canonical representation for each pair (ignore direction for counting pairs)
        gap_df["pair"] = gap_df.apply(
            lambda row: frozenset({row["neuron1"], row["neuron2"]}), axis=1
        )
        num_gap_pairs = gap_df["pair"].nunique()
        # Calculate average gap weight over unique pairs to avoid double counting
        avg_gap_weight = (
            gap_df.drop_duplicates(subset=["pair"])["mean_gap_weight"].mean()
            if num_gap_pairs > 0
            else np.nan
        )

        print(f"Neuron Pairs with Gap Junctions: {num_gap_pairs}")
        print(f"Average Gap Junction Weight (Symmetric): {avg_gap_weight:.3f}")

        # Chemical Synapses (Based on directed edges)
        chem_df = summary_df.dropna(subset=["mean_chem_weight"])
        num_chem_edges = len(chem_df)
        avg_chem_weight = (
            chem_df["mean_chem_weight"].mean() if num_chem_edges > 0 else np.nan
        )

        print(f"\nDirected Neuron Pairs with Chemical Synapses: {num_chem_edges}")
        print(f"Average Chemical Synapse Weight (Directed): {avg_chem_weight:.3f}")

        # Functional Relationships (Based on directed edges)
        func_df = summary_df.dropna(subset=["functional_weight"])
        num_func_edges = len(func_df)
        avg_func_weight = (
            func_df["functional_weight"].mean() if num_func_edges > 0 else np.nan
        )

        print(
            f"\nDirected Neuron Pairs with Functional Relationships (from funconn): {num_func_edges}"
        )
        print(
            f"Average Functional Relationship Weight (Directed): {avg_func_weight:.3f}"
        )

        # Save df as CSV
        output_csv_path = os.path.join(PROCESSED_DATA_DIR, "aggregated_connectome.csv")
        try:
            summary_df.to_csv(output_csv_path, index=False)
            print(f"\nSummary DataFrame saved to: {output_csv_path}")
        except Exception as e:
            print(f"\nError saving summary DataFrame to CSV: {e}")
    else:
        print("\nAnalysis finished, but no data was aggregated.")