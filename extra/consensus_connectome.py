"""
Analyzes and synthesizes pre-processed connectome data into a single CSV file
called data/processed/aggregated_connectome.csv.

This script reads pre-processed connectome files (graph_tensors_*.pt) generated by
preprocess.py -c and combines their data into an aggregated_connectome.csv file.
It extracts connection information from each tensor file, calculates statistics
for gap junctions, chemical synapses, and functional weights, and merges them.

The output CSV contains the following columns:
- from_neuron, to_neuron: The pair of neurons (directed from from_neuron to to_neuron)
- from_pos_x, from_pos_y, from_pos_z: Position coordinates of from_neuron
- to_pos_x, to_pos_y, to_pos_z: Position coordinates of to_neuron
- from_type, to_type: Type of from_neuron and to_neuron
- mean_gap_weight: Average gap junction weight (symmetric)
- mean_chem_weight: Average chemical synapse weight (directed)
- functional_weight: Functional connectivity weight from the funconn dataset
- data_sources: List of source datasets that contributed to this connection
"""

import torch
import pandas as pd
import numpy as np
import os
import glob
import sys
from collections import defaultdict

# --- Configuration ---
# Add the parent directory of 'preprocess' to sys.path so we can import preprocess._pkg
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
from preprocess._pkg import NEURON_LABELS

# --- Load neuron metadata ---
# Load neuron_master_sheet.csv for position/type lookup
NEURON_META_PATH = os.path.join(PROJECT_ROOT, "data", "raw", "neuron_master_sheet.csv")
df_neuron_meta = pd.read_csv(NEURON_META_PATH)
df_neuron_meta = df_neuron_meta.set_index("label")

PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT, "data/processed/connectome")


# --- Helper Mappings ---
if not NEURON_LABELS:
    raise ValueError("NEURON_LABELS list is empty. Cannot proceed.")
NEURON_TO_IDX = {label: idx for idx, label in enumerate(NEURON_LABELS)}
IDX_TO_LABEL = {idx: label for label, idx in NEURON_TO_IDX.items()}
NUM_NODES = len(NEURON_LABELS)

# --- Functions ---

def get_dataset_name(filepath: str) -> str:
    """Extracts a base name from the .pt filepath for use as data source identifier."""
    base = os.path.basename(filepath)
    name = base.replace("graph_tensors_", "").replace(".pt", "")
    return name

def analyze_all_connectomes(data_dir: str) -> pd.DataFrame:
    """
    Loads all processed .pt connectome files, aggregates connection data,
    and returns a summary DataFrame.

    Args:
        data_dir: Path to the directory containing the .pt files.

    Returns:
        A Pandas DataFrame summarizing connections across datasets.
        Columns: from_neuron, to_neuron, from_pos_x, from_pos_y, from_pos_z,
                 to_pos_x, to_pos_y, to_pos_z, from_type, to_type, data_sources,
                 mean_gap_weight, mean_chem_weight, functional_weight.
    """
    pt_files = glob.glob(os.path.join(data_dir, "graph_tensors_*.pt"))
    if not pt_files:
        print(f"Warning: No 'graph_tensors_*.pt' files found in {data_dir}")
        return pd.DataFrame(
            columns=[
                "from_neuron",
                "to_neuron",
                "from_pos",
                "to_pos",
                "from_type",
                "to_type",
                "mean_gap_weight",
                "mean_chem_weight",
                "functional_weight",
                "data_sources",
            ]
        )

    # Store collected data per DIRECTED pair (from_neuron_label, to_neuron_label)
    collected_data = defaultdict(lambda: {
        "gap_vals_A_to_B": [],
        "gap_vals_B_to_A": [],
        "chem_vals_A_to_B": [],
        "func_val_A_to_B": np.nan,
        "gap_sources": set(),
        "chem_sources": set(),
        "func_sources": set(),
    })

    print(f"Found {len(pt_files)} potential connectome files to analyze.")

    # --- Step 1: Load data and collect raw values per directed edge ---
    for filepath in pt_files:
        dataset_name = get_dataset_name(filepath)
        print(f"Loading: {dataset_name} ({os.path.basename(filepath)})")
        try:
            data = torch.load(filepath, map_location=torch.device('cpu')) # Load to CPU
            edge_index = data.get('edge_index')
            edge_attr = data.get('edge_attr')

            if edge_index is None or edge_attr is None:
                print(f"  Skipping {dataset_name}: Missing 'edge_index' or 'edge_attr'.")
                continue
            if edge_attr.shape[1] < 2:
                 print(f"  Skipping {dataset_name}: edge_attr has shape {edge_attr.shape}, expected at least 2 features.")
                 continue

            # Create an efficient lookup for edges in this specific file
            # Assumes indices in the file match global NEURON_TO_IDX
            edge_lookup = {
                (u.item(), v.item()): i
                for i, (u, v) in enumerate(edge_index.t())
            }

            # Iterate through all possible neuron pairs using global indices
            for idx1 in range(NUM_NODES):
                for idx2 in range(NUM_NODES):
                    # Get labels for the current pair
                    label1 = IDX_TO_LABEL.get(idx1)
                    label2 = IDX_TO_LABEL.get(idx2)
                    if not label1 or not label2:
                        continue  # Should not happen

                    pair_key = (label1, label2) # Directed key

                    # Check if the directed edge A->B exists in this dataset
                    edge_map_idx = edge_lookup.get((idx1, idx2))
                    if edge_map_idx is not None:
                        attr = edge_attr[edge_map_idx]
                        gap_w = attr[0].item()
                        chem_w = attr[1].item()

                        if gap_w != 0:
                            collected_data[pair_key]["gap_vals_A_to_B"].append(gap_w)
                            collected_data[pair_key]["gap_sources"].add(dataset_name)
                        if chem_w != 0:
                            # Store both value and dataset name for chem weights
                            collected_data[pair_key]["chem_vals_A_to_B"].append((chem_w, dataset_name))
                            collected_data[pair_key]["chem_sources"].add(dataset_name)

                        # Special handling for functional connectivity dataset
                        if dataset_name == "funconn" and chem_w != 0:
                            collected_data[pair_key]["func_val_A_to_B"] = chem_w
                            collected_data[pair_key]["func_sources"].add(dataset_name)

                    # Also check the reverse edge B->A to collect its gap weight for symmetric calculation later
                    edge_map_idx_rev = edge_lookup.get((idx2, idx1))
                    if edge_map_idx_rev is not None:
                        attr_rev = edge_attr[edge_map_idx_rev]
                        gap_w_rev = attr_rev[0].item()
                        if gap_w_rev != 0:
                             collected_data[pair_key]["gap_vals_B_to_A"].append(gap_w_rev)
                             collected_data[pair_key]["gap_sources"].add(dataset_name)


        except Exception as e:
            print(f"  Error processing {dataset_name}: {e}")

    # --- Step 2: Aggregate collected data into final DataFrame rows ---
    output_rows = []

    print("\nAggregating results...")
    # Iterate through all N*N pairs again to ensure all are included
    for label1 in NEURON_LABELS:
        for label2 in NEURON_LABELS:
            # include self-loops
            pair_key_AB = (label1, label2)
            data_AB = collected_data[pair_key_AB]
            # Combine all gap values related to the pair {A, B} from both directions
            all_gap_values = data_AB["gap_vals_A_to_B"] + data_AB["gap_vals_B_to_A"]
            # We might have duplicates if a dataset correctly listed symmetric values, use unique values
            unique_gap_values = list(set(all_gap_values))

            mean_gap = np.mean(unique_gap_values) if unique_gap_values else np.nan

            # Only use chemical weights from non-funconn datasets
            chem_weights_non_funconn = [
                w for (w, ds) in data_AB["chem_vals_A_to_B"] if ds != "funconn"
            ]
            chem_sources = data_AB["chem_sources"]
            if not chem_weights_non_funconn:
                mean_chem_AB = np.nan
            else:
                mean_chem_AB = np.mean(chem_weights_non_funconn)

            func_AB = data_AB["func_val_A_to_B"]
            sources_AB = sorted(
                data_AB["gap_sources"] | data_AB["chem_sources"] | data_AB["func_sources"]
            )

            # --- Lookup metadata for both neurons ---
            def get_meta(label):
                if label in df_neuron_meta.index:
                    row = df_neuron_meta.loc[label]
                    # Convert to regular floats for CSV output
                    pos = (float(row["x"]), float(row["y"]), float(row["z"]))
                    typ = row["type"]
                    return pos, typ
                else:
                    return (np.nan, np.nan, np.nan), None
            from_pos, from_type = get_meta(label1)
            to_pos, to_type = get_meta(label2)

            output_rows.append({
                "from_neuron": label1,
                "to_neuron": label2,
                "from_pos": from_pos,
                "to_pos": to_pos,
                "from_type": from_type,
                "to_type": to_type,
                "mean_gap_weight": mean_gap, # Symmetric mean
                "mean_chem_weight": mean_chem_AB, # Directed mean A->B, excluding funconn
                "functional_weight": func_AB, # Directed A->B
                "data_sources": sources_AB
            })

    print(f"Created {len(output_rows)} summary rows.")
    if not output_rows:
        return pd.DataFrame(
            columns=[
                "from_neuron", "to_neuron",
                "from_pos", "to_pos",
                "from_type", "to_type",
                "mean_gap_weight", "mean_chem_weight", "functional_weight", "data_sources"
            ]
        )

    return pd.DataFrame(output_rows)


if __name__ == "__main__":
    print("Starting connectome analysis...")
    # Prompt user for excluding null rows
    while True:
        user_input = input("Exclude rows where all weights are null (gap, chem, functional)? [y/N]: ").strip().lower()
        if user_input in ('y', 'n', ''):
            break
    exclude_null = (user_input == 'y')
    summary_df = analyze_all_connectomes(PROCESSED_DATA_DIR)

    if not summary_df.empty:
        print("\nAnalysis Complete. DataFrame head:")
        print(summary_df.head(100))
        
        # --- Calculate and Print Statistics ---
        print("\n--- Overall Statistics ---")
        print(f"Total Neuron Pairs:{len(summary_df)}\n")

        # Gap Junctions (Based on unique pairs)
        gap_df = summary_df.dropna(subset=["mean_gap_weight"])
        # Create a canonical representation for each pair (ignore direction for counting pairs)
        gap_df["pair"] = gap_df.apply(
            lambda row: frozenset({row["from_neuron"], row["to_neuron"]}), axis=1
        )
        num_gap_pairs = gap_df["pair"].nunique()
        # Calculate average gap weight over unique pairs to avoid double counting
        avg_gap_weight = (
            gap_df.drop_duplicates(subset=["pair"])["mean_gap_weight"].mean()
            if num_gap_pairs > 0
            else np.nan
        )

        print(f"Neuron Pairs with Gap Junctions: {num_gap_pairs}")
        print(f"Average Gap Junction Weight (Symmetric): {avg_gap_weight:.3f}")

        # Chemical Synapses (Based on directed edges)
        chem_df = summary_df.dropna(subset=["mean_chem_weight"])
        num_chem_edges = len(chem_df)
        avg_chem_weight = (
            chem_df["mean_chem_weight"].mean() if num_chem_edges > 0 else np.nan
        )

        print(f"\nDirected Neuron Pairs with Chemical Synapses: {num_chem_edges}")
        print(f"Average Chemical Synapse Weight (Directed): {avg_chem_weight:.3f}")

        # Functional Relationships (Based on directed edges)
        func_df = summary_df.dropna(subset=["functional_weight"])
        num_func_edges = len(func_df)
        avg_func_weight = (
            func_df["functional_weight"].mean() if num_func_edges > 0 else np.nan
        )

        print(
            f"\nDirected Neuron Pairs with Functional Relationships (from funconn): {num_func_edges}"
        )
        print(
            f"Average Functional Relationship Weight (Directed): {avg_func_weight:.3f}"
        )

        if exclude_null:
            # Exclude rows where all weights are null
            mask = ~(
                summary_df['mean_gap_weight'].isnull() &
                summary_df['mean_chem_weight'].isnull() &
                summary_df['functional_weight'].isnull()
            )
            trimmed_df = summary_df[mask].copy()
            output_csv_path = os.path.join(PROJECT_ROOT, "datasets", "consensus_connectome_not_null.csv")
            trimmed_df.to_csv(output_csv_path, index=False)
            print(f"\nTrimmed DataFrame saved to: {output_csv_path}")
        else:
            output_csv_path = os.path.join(PROJECT_ROOT, "datasets", "consensus_connectome_full.csv")
            summary_df.to_csv(output_csv_path, index=False)
            print(f"\nSummary DataFrame saved to: {output_csv_path}")
    else:
        print("\nAnalysis finished, but no data was aggregated.")